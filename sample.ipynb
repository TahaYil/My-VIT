{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T09:08:34.206716Z",
     "start_time": "2025-07-26T09:08:32.064699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:40:57.897163Z",
     "start_time": "2025-07-26T08:40:57.893861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class embendingLayer(nn.Module):\n",
    "    def __init__(self,in_channels:int=3,\n",
    "                 embed_dim:int=768,\n",
    "                 patch_size:int=16\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.patcher=nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=embed_dim,\n",
    "                               kernel_size=patch_size,\n",
    "                               stride=patch_size,\n",
    "                               padding=0)\n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        img_res=x.shape[-1]\n",
    "        assert img_res % self.patch_size ==0 , f\"must be divisible\"\n",
    "\n",
    "        x=self.flatten(self.patcher(x))\n",
    "        return x.permute(0,2,1)"
   ],
   "id": "b303257f7892aaac",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:40:58.436029Z",
     "start_time": "2025-07-26T08:40:58.433498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class msa_block(nn.Module):\n",
    "    def __init__(self,embed_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 dropout:float=0.0,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        #NORMALLY WE RE USING LINEAR LAYER FOR GENERATING Q K V !!!!! \n",
    "        \n",
    "        self.norm=nn.LayerNorm(normalized_shape= embed_dim)\n",
    "        \n",
    "        self.query=nn.Linear(in_features=embed_dim,out_features=embed_dim)\n",
    "        self.key=nn.Linear(in_features=embed_dim,out_features=embed_dim)\n",
    "        self.value=nn.Linear(in_features=embed_dim,out_features=embed_dim)\n",
    "        \n",
    "        self.msa=nn.MultiheadAttention(embed_dim,num_heads=num_heads,dropout=dropout,batch_first=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.norm(x)\n",
    "        q=self.query(x)\n",
    "        k=self.key(x)\n",
    "        v=self.value(x)\n",
    "        attn_out,_=self.msa(q,k,v,need_weights=False)\n",
    "        return attn_out"
   ],
   "id": "c842120052fa7dda",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:40:58.995370Z",
     "start_time": "2025-07-26T08:40:58.992245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class mlp_block(nn.Module):\n",
    "    def __init__(self,embed_dim:int=768,\n",
    "                 mlp_size:int=3078,\n",
    "                 dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.norm=nn.LayerNorm(normalized_shape= embed_dim)\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim,out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=mlp_size,out_features=embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.mlp(self.norm(x))"
   ],
   "id": "bf73695c2de9ba1e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:40:59.595596Z",
     "start_time": "2025-07-26T08:40:59.525812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer_encoder(nn.Module):\n",
    "    def __init__(self,embed_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3078,\n",
    "                 mlp_dropout:float=0.1,\n",
    "                 msa_dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        self.msa=msa_block(embed_dim=embed_dim,\n",
    "                           num_heads=num_heads,\n",
    "                           dropout=msa_dropout)\n",
    "        self.mlp=mlp_block(embed_dim=embed_dim,\n",
    "                           mlp_size=mlp_size,\n",
    "                           dropout=mlp_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.msa(x)+x #Resudual connections\n",
    "        x=self.mlp(x)+x\n",
    "        return x\n",
    "    \n",
    "encoder=Transformer_encoder(embed_dim=768,\n",
    "                            num_heads=12,\n",
    "                            mlp_size=3078,\n",
    "                            mlp_dropout=0.1,\n",
    "                            msa_dropout=0.0)\n",
    "summary(model=encoder,\n",
    "        input_size=(1,197,768),\n",
    "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ],
   "id": "b14d9f9db1ac2d85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "Transformer_encoder (Transformer_encoder)     [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "├─msa_block (msa)                             [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─LayerNorm (norm)                       [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "│    └─Linear (query)                         [1, 197, 768]        [1, 197, 768]        590,592              True\n",
       "│    └─Linear (key)                           [1, 197, 768]        [1, 197, 768]        590,592              True\n",
       "│    └─Linear (value)                         [1, 197, 768]        [1, 197, 768]        590,592              True\n",
       "│    └─MultiheadAttention (msa)               [1, 197, 768]        [1, 197, 768]        2,362,368            True\n",
       "├─mlp_block (mlp)                             [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    └─LayerNorm (norm)                       [1, 197, 768]        [1, 197, 768]        1,536                True\n",
       "│    └─Sequential (mlp)                       [1, 197, 768]        [1, 197, 768]        --                   True\n",
       "│    │    └─Linear (0)                        [1, 197, 768]        [1, 197, 3078]       2,366,982            True\n",
       "│    │    └─GELU (1)                          [1, 197, 3078]       [1, 197, 3078]       --                   --\n",
       "│    │    └─Dropout (2)                       [1, 197, 3078]       [1, 197, 3078]       --                   --\n",
       "│    │    └─Linear (3)                        [1, 197, 3078]       [1, 197, 768]        2,364,672            True\n",
       "│    │    └─Dropout (4)                       [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "=============================================================================================================================\n",
       "Total params: 8,868,870\n",
       "Trainable params: 8,868,870\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 6.51\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.61\n",
       "Forward/backward pass size (MB): 12.11\n",
       "Params size (MB): 26.03\n",
       "Estimated Total Size (MB): 38.74\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:41:00.566285Z",
     "start_time": "2025-07-26T08:41:00.541967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pytorch_encoder=nn.TransformerEncoderLayer(d_model=768,\n",
    "                                           nhead=12,\n",
    "                                           dim_feedforward=3078,\n",
    "                                           dropout=0.1,\n",
    "                                           activation=\"gelu\",\n",
    "                                           batch_first=True,\n",
    "                                           norm_first=True)"
   ],
   "id": "67fe5ccd5c317108",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:41:01.202685Z",
     "start_time": "2025-07-26T08:41:01.195150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class My_VIT(nn.Module):\n",
    "    def __init__(self,embed_dim:int=768,\n",
    "                 img_size:int=224,\n",
    "                 patch_size:int=16,\n",
    "                 in_channels:int=3,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3078,\n",
    "                 mlp_dropout:float=0.1,\n",
    "                 msa_dropout:float=0.0,\n",
    "                 embending_dropout:float=0.1,\n",
    "                 num_transformer_layers:int=12,\n",
    "                 num_classes:int=1000):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0 , f\"must be divisible\"\n",
    "        self.num_patches=img_size**2//patch_size**2\n",
    "        self.class_token=nn.Parameter(torch.randn(1,1,embed_dim),\n",
    "                                      requires_grad=True)\n",
    "        self.position_embed=nn.Parameter(torch.randn(1,self.num_patches+1,embed_dim),\n",
    "                                   requires_grad=True)\n",
    "        self.patch_embed=embendingLayer(in_channels=in_channels,\n",
    "                                        embed_dim=embed_dim,\n",
    "                                        patch_size=patch_size)\n",
    "        self.embed_dropout=nn.Dropout(embending_dropout)\n",
    "        self.transformer_encoder=nn.Sequential(*[Transformer_encoder(embed_dim=embed_dim,\n",
    "                                                       mlp_size=mlp_size,\n",
    "                                                       mlp_dropout=mlp_dropout,\n",
    "                                                       msa_dropout=msa_dropout,\n",
    "                                                       num_heads=num_heads) for _ in range(num_transformer_layers) ])\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(normalized_shape= embed_dim),\n",
    "                                        nn.Linear(in_features=embed_dim,out_features=num_classes))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size=x.shape[0]\n",
    "        class_token=self.class_token.expand(batch_size,-1,-1) # Adding batch size head of the token \n",
    "        x=self.patch_embed(x)\n",
    "        x=torch.cat((class_token,x),dim=1)\n",
    "        x=self.position_embed+x #we don t need to add batch cause pytorch automatichly doing broadcast\n",
    "        x=self.embed_dropout(x)\n",
    "        x=self.transformer_encoder(x)\n",
    "        x = self.classifier(x[:,0]) # we re using only class token for classifier\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ],
   "id": "41e9867249371ccd",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:41:05.026398Z",
     "start_time": "2025-07-26T08:41:04.322898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vit=My_VIT()\n",
    "deneme=torch.randn((1,3,224,224))\n",
    "res=vit(deneme)\n",
    "print(res.shape)"
   ],
   "id": "8f449578b7537b22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T08:41:15.855544Z",
     "start_time": "2025-07-26T08:41:15.843034Z"
    }
   },
   "cell_type": "code",
   "source": "print(res)",
   "id": "b2ac963e639750a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.2701e-01, -1.4336e-01,  1.4696e-01,  2.0628e-01,  1.1318e+00,\n",
      "          6.5189e-01,  4.6985e-01,  6.7642e-01,  1.0279e+00,  6.0500e-01,\n",
      "          9.3748e-01, -1.3748e+00, -6.3294e-01,  2.3155e-01,  3.1434e-01,\n",
      "         -7.9062e-01,  7.9653e-01, -2.5648e-01,  3.9556e-02, -9.8630e-01,\n",
      "          2.2320e-01, -1.5466e+00, -5.7689e-01, -7.5966e-01, -5.4603e-01,\n",
      "         -3.6575e-02,  8.6149e-01,  7.0976e-01, -2.8751e-01, -3.5410e-01,\n",
      "          4.8861e-01, -4.4966e-01,  1.6824e-01,  3.2294e-01,  1.1305e+00,\n",
      "          1.2078e-01, -6.1001e-01,  1.3427e+00, -5.5469e-01, -7.0903e-01,\n",
      "         -1.2707e-01,  7.3911e-01,  5.3180e-01,  8.3107e-01,  6.9507e-02,\n",
      "          1.0712e+00,  1.2208e+00,  8.6902e-01, -5.0508e-01,  1.0589e+00,\n",
      "          1.6623e+00, -6.7132e-02, -1.7442e+00, -1.6153e-03,  4.3672e-01,\n",
      "         -8.6881e-01,  6.8427e-01, -6.2214e-01,  2.0280e-01, -1.2325e+00,\n",
      "          2.8699e-01,  1.3368e-01,  8.1792e-01,  1.9831e-02,  3.6766e-01,\n",
      "          3.1157e-01,  8.7006e-01,  1.9989e-01,  5.9026e-01, -5.8504e-01,\n",
      "          4.2076e-01, -2.4330e-01, -5.9670e-01, -7.2396e-01, -2.3963e-01,\n",
      "          9.7948e-02, -6.4807e-01,  7.0973e-01, -8.9191e-02, -1.3505e+00,\n",
      "          5.0050e-01,  1.1776e-01, -8.1240e-01,  1.9877e-01, -8.3008e-01,\n",
      "          1.0345e+00, -7.4684e-01,  7.2667e-01,  1.5311e+00,  2.0771e-01,\n",
      "          2.6222e-01,  2.4910e-01,  9.6831e-01, -1.0025e+00, -6.8065e-01,\n",
      "         -2.2019e-01, -2.1774e-01,  5.3954e-01,  1.3795e-01,  5.5295e-01,\n",
      "          4.4583e-01, -7.1224e-01,  3.0782e-02, -2.7989e-01,  1.0136e+00,\n",
      "         -4.8863e-01,  1.7662e-01,  3.6237e-01,  8.4958e-01,  1.6082e-01,\n",
      "         -4.6214e-01,  7.3435e-02,  5.0792e-01, -1.3955e-01,  4.8832e-01,\n",
      "         -3.0577e-01,  8.0725e-01, -7.5478e-01, -1.4257e-01, -1.5006e-01,\n",
      "         -1.1555e+00, -3.2303e-01,  2.1923e-01,  2.2296e-01, -7.2002e-01,\n",
      "         -2.4823e-01, -5.1684e-01,  2.4120e-01,  4.7583e-01, -3.0973e-02,\n",
      "          2.1946e-02, -3.1520e-01,  5.5710e-01, -6.7078e-01, -4.7173e-01,\n",
      "          3.5141e-01, -7.9845e-01, -4.2527e-01, -5.5819e-01, -6.5121e-01,\n",
      "         -5.0553e-01,  1.6890e-01,  1.5766e-01, -4.4679e-01, -1.3903e-02,\n",
      "         -4.0918e-01,  1.7108e-01, -5.1866e-01, -6.7961e-02,  1.3985e-01,\n",
      "          3.1926e-01,  9.8881e-02,  4.9241e-01,  6.2464e-02, -5.1944e-01,\n",
      "         -4.4987e-02, -1.2882e+00,  1.2223e+00,  1.2582e-01, -3.7317e-01,\n",
      "         -9.0053e-01, -1.0792e+00,  3.2016e-01, -2.5508e-02,  1.3562e+00,\n",
      "          3.6163e-01,  6.4163e-01,  5.5407e-01, -3.7928e-02,  2.0690e-01,\n",
      "          6.4170e-01, -8.4299e-01, -4.5502e-02,  2.8501e-01,  9.8912e-01,\n",
      "         -6.5339e-01, -8.1304e-01,  3.8232e-01,  3.5476e-02,  6.4164e-01,\n",
      "          2.7541e-01,  7.0709e-02,  1.1057e+00, -5.0419e-01,  4.0515e-01,\n",
      "         -3.6576e-01,  1.1880e-01,  3.1852e-01,  1.2338e-02,  1.3242e+00,\n",
      "         -4.4251e-01,  4.7310e-01,  7.1804e-01, -2.3809e-01, -5.9677e-01,\n",
      "         -3.7476e-02,  4.1812e-01,  3.8270e-01, -5.2272e-01,  2.7988e-01,\n",
      "         -9.2171e-02, -4.9309e-01, -9.2845e-01, -4.4723e-02, -3.2253e-01,\n",
      "          2.8025e-02, -3.3545e-01,  1.5231e-01,  5.1608e-01,  7.0527e-01,\n",
      "          2.9200e-01,  9.3567e-01, -3.4762e-01, -1.4759e-02, -7.3803e-01,\n",
      "         -2.9056e-01, -1.0598e-01, -6.8182e-01,  9.4772e-01,  8.1190e-01,\n",
      "         -1.1376e+00, -5.9730e-01, -3.5624e-01,  4.4063e-01, -3.9513e-01,\n",
      "         -3.3392e-01, -2.1371e-01,  1.5278e-01, -2.2537e-01,  6.9859e-02,\n",
      "          7.2914e-01,  2.6244e-01, -1.8745e-01, -6.1973e-01,  3.2935e-01,\n",
      "          3.4002e-01,  4.9599e-02,  1.1858e+00,  2.2279e-01,  2.9426e-01,\n",
      "         -6.0210e-01,  1.1455e-01,  3.7383e-02, -4.9925e-01, -6.5960e-01,\n",
      "         -1.8260e-01, -3.3179e-01, -2.6432e-01, -3.8274e-01,  4.8414e-01,\n",
      "          1.6831e-01,  3.7144e-01, -3.0269e-01, -2.8897e-01, -8.8127e-01,\n",
      "         -4.8393e-01,  2.1476e-01, -4.0848e-01,  8.2621e-02,  1.0100e+00,\n",
      "         -4.3591e-01, -1.6500e-01,  4.8172e-01,  4.8523e-01, -5.8541e-01,\n",
      "          3.7623e-01,  1.3406e+00,  1.8847e-01, -1.0449e+00, -2.2454e-01,\n",
      "          6.9727e-01, -3.3757e-01, -2.6886e-02,  2.8820e-01, -4.2893e-01,\n",
      "          1.2202e-01, -1.2492e+00,  2.0826e+00, -1.5672e-01,  2.9729e-01,\n",
      "         -8.3652e-01,  3.1624e-01,  8.9130e-01,  3.1790e-01,  8.5841e-01,\n",
      "         -1.1625e+00,  1.2703e-01, -6.5180e-01,  1.9481e-01,  5.1141e-02,\n",
      "         -1.2036e-01, -2.6920e-01, -4.1908e-01,  1.6211e-01,  9.8643e-02,\n",
      "         -4.0377e-01, -4.1575e-01,  3.1027e-01,  6.3553e-01, -1.1129e+00,\n",
      "         -2.9155e-01,  1.2114e-01, -1.1598e+00,  5.6395e-01, -3.8618e-01,\n",
      "          1.9616e-01, -5.3678e-01, -9.8825e-01, -5.8430e-02,  5.1210e-01,\n",
      "         -6.7194e-01,  3.0156e-01, -2.5672e-01,  3.7647e-01, -1.1944e+00,\n",
      "          1.0952e+00, -9.0121e-01,  2.5110e-01,  5.5573e-01,  7.8759e-02,\n",
      "         -4.6283e-01, -2.9036e-01,  3.9500e-02, -2.4946e-01, -2.9100e-01,\n",
      "          3.5019e-01,  2.3364e-01, -8.2581e-02,  6.6525e-02, -5.9525e-01,\n",
      "         -1.1749e-01, -1.2377e-01, -1.5306e-01, -7.6071e-01,  2.5319e-01,\n",
      "         -1.7247e-01, -4.1291e-01, -1.2297e+00, -1.4075e-01, -2.6540e-02,\n",
      "         -3.9702e-01, -2.6272e-01,  1.3071e+00,  1.1492e+00, -8.0193e-01,\n",
      "         -1.6392e-01, -2.1134e-01,  7.3971e-01, -7.1682e-01,  9.0629e-01,\n",
      "         -1.0431e+00, -3.1076e-02, -3.4089e-01, -1.5654e-01,  8.1757e-01,\n",
      "         -2.9808e-01, -1.3303e+00, -1.1310e-01, -6.5187e-01, -9.8597e-01,\n",
      "         -6.2741e-01,  2.3532e-01, -2.6459e-01,  9.9740e-01, -3.0791e-01,\n",
      "          5.3192e-03,  1.1698e-01, -1.4968e-01, -1.4860e-01, -5.8434e-01,\n",
      "         -2.1891e-01,  4.1508e-01,  6.6696e-01, -1.6313e-02, -4.5488e-01,\n",
      "         -3.9146e-01,  1.1712e+00, -1.2540e-01,  5.4487e-01, -6.4929e-01,\n",
      "         -1.1173e+00,  5.8670e-01, -6.2324e-01,  6.0987e-01,  5.4688e-01,\n",
      "          1.5692e-01,  3.7754e-01,  3.4137e-01,  2.7926e-01,  3.8261e-01,\n",
      "         -8.5973e-02, -7.6937e-03, -2.7936e-01,  9.9218e-01, -1.0262e+00,\n",
      "          6.1146e-01, -2.9131e-01,  7.8279e-01, -1.6857e-01, -3.5442e-01,\n",
      "         -5.1593e-01,  6.4963e-01, -3.6217e-01, -8.3419e-02,  5.9463e-01,\n",
      "         -1.9499e-01, -1.8901e-01, -2.3105e-01, -1.2230e-01, -6.3583e-01,\n",
      "          1.6190e-01, -2.6713e-01, -1.4812e-01, -2.7450e-01, -4.6048e-01,\n",
      "          1.5532e-01,  6.8865e-01, -4.6839e-02, -3.8644e-01,  6.7197e-01,\n",
      "          2.3115e-01, -4.5137e-01, -7.9288e-01,  6.6367e-01, -8.6936e-01,\n",
      "          5.4880e-01, -4.0423e-01,  4.5501e-01,  5.1996e-01,  5.1215e-02,\n",
      "         -4.2963e-01, -5.2410e-01,  4.6773e-01,  8.4744e-01,  4.4358e-01,\n",
      "          8.8098e-01,  1.3431e-01,  4.8202e-01, -2.1797e-01,  6.6004e-01,\n",
      "          7.5270e-02,  5.1358e-02,  5.3258e-01, -2.5955e-02, -5.5767e-01,\n",
      "         -5.6350e-01, -1.3922e-01,  7.7068e-02, -1.3373e-01, -1.5622e-01,\n",
      "          1.5767e-01, -9.8263e-02,  2.8154e-01,  4.9766e-01,  3.2617e-01,\n",
      "         -1.4431e-02,  2.4412e-01, -7.8026e-01,  5.8011e-01,  3.8053e-02,\n",
      "          4.1085e-01,  1.8584e-01, -2.7235e-01,  4.8626e-01,  5.0064e-01,\n",
      "         -6.2163e-02, -6.8018e-02,  3.1774e-01,  1.9408e-01,  5.7225e-02,\n",
      "         -4.0360e-01, -3.7787e-01,  1.9157e-04, -1.7125e-02,  4.7670e-01,\n",
      "          4.5266e-01,  3.6349e-01,  2.7960e-01, -2.9208e-01,  1.9995e-01,\n",
      "          3.3570e-01,  2.5749e-01,  3.1104e-01,  1.4010e+00, -5.9414e-01,\n",
      "         -9.6719e-01, -5.1200e-01, -3.1904e-01, -5.2720e-01, -7.3549e-01,\n",
      "         -2.8236e-01, -6.8373e-02, -2.0790e-01,  2.7198e-02, -4.4669e-01,\n",
      "          4.6511e-01,  2.0607e-01, -1.1113e+00, -7.2347e-01,  5.5626e-01,\n",
      "          1.4512e-01,  6.2801e-01,  1.0377e-01,  6.0293e-01,  5.4151e-01,\n",
      "          9.0821e-02, -8.9715e-02, -2.7533e-01, -2.2118e-02, -1.7810e-01,\n",
      "          3.1314e-01,  9.1172e-01,  7.7520e-01, -5.6891e-02,  2.4500e-02,\n",
      "         -4.9671e-01, -1.2076e+00,  4.6857e-01,  2.3563e-01, -3.2950e-01,\n",
      "         -8.2137e-01, -4.6529e-01, -5.8717e-01, -2.4710e-01,  3.5256e-01,\n",
      "          2.4570e-01,  4.3774e-01,  7.6295e-01,  6.3411e-02, -6.9316e-01,\n",
      "         -1.7229e-01,  8.2495e-01,  3.9643e-01,  9.8017e-03,  2.1559e-01,\n",
      "         -1.5904e-02, -7.6785e-01, -4.6171e-01, -6.6148e-02, -6.2830e-02,\n",
      "          1.4536e-01,  1.2303e+00, -4.5800e-01,  1.3346e-01, -7.6536e-01,\n",
      "         -2.3032e-01, -3.2866e-01, -5.2741e-01,  7.9087e-01,  8.3710e-02,\n",
      "         -8.4862e-02, -4.8970e-01, -2.6065e-01,  5.6634e-01, -2.9781e-01,\n",
      "          3.6543e-01, -5.0308e-01,  1.4837e-01,  5.2656e-01, -9.7206e-02,\n",
      "         -1.0301e+00, -7.2559e-01, -1.0814e+00,  4.9850e-01,  9.3295e-01,\n",
      "          1.0432e+00,  2.3334e-01, -4.3688e-01,  3.1694e-01,  6.6325e-02,\n",
      "          6.0240e-01,  2.2273e-01, -7.5443e-02,  2.8439e-02, -1.4531e-01,\n",
      "         -3.0694e-02,  3.3983e-01,  5.2964e-01, -3.3132e-01, -2.2787e-02,\n",
      "         -3.0545e-01,  4.8359e-01, -7.6571e-01, -9.1728e-01,  4.3615e-01,\n",
      "          1.4213e+00,  8.9131e-01,  1.3005e-01, -7.8376e-01, -3.6225e-01,\n",
      "         -1.5095e-01, -2.8275e-01,  5.8914e-01,  1.6057e-01, -4.2759e-01,\n",
      "         -2.0582e-01, -2.3102e-01, -7.4101e-01, -1.4583e-01, -4.0238e-01,\n",
      "         -2.9899e-02,  3.8034e-02, -1.2278e-01,  1.0002e-01,  3.0931e-01,\n",
      "          7.5282e-01,  1.9573e-01,  6.7201e-01, -1.3406e-02,  6.8858e-01,\n",
      "         -4.9124e-01,  7.9928e-01, -5.4135e-02, -3.3606e-01, -5.6190e-01,\n",
      "          6.0924e-01, -2.9356e-02, -2.8784e-01,  6.8135e-01,  2.5627e-01,\n",
      "          4.6577e-01, -2.5621e-01,  3.6944e-01, -6.2035e-01, -1.0686e-01,\n",
      "         -1.9022e-02,  7.4501e-02,  4.1033e-01, -7.4635e-01,  4.5838e-01,\n",
      "          3.2563e-01,  4.6045e-01,  6.2998e-01, -2.0264e-01, -1.2209e-01,\n",
      "         -6.4698e-01,  8.1518e-01,  8.1032e-01,  5.5598e-01,  4.6125e-01,\n",
      "          4.6740e-01,  4.8096e-01,  7.8620e-01, -1.1535e-01,  2.4503e-01,\n",
      "         -5.3089e-01, -4.8521e-01,  1.4492e-01, -2.5189e-01, -1.3960e-01,\n",
      "         -7.0523e-02, -3.6844e-01, -3.7698e-01, -1.0197e-01, -4.0503e-01,\n",
      "         -1.2209e+00,  2.0020e-01,  3.1756e-02, -5.7144e-03, -1.4385e+00,\n",
      "         -7.3881e-01, -5.2264e-01, -2.7491e-01,  1.1454e-01,  8.6641e-01,\n",
      "          9.0494e-02,  1.1881e-01, -1.1146e-01, -7.5785e-01, -5.3354e-01,\n",
      "          3.4712e-01,  5.0672e-01,  2.3734e-01, -4.7438e-02, -7.7951e-02,\n",
      "          7.3553e-02,  2.0900e-01, -6.8732e-01,  6.3917e-01,  8.7738e-01,\n",
      "         -5.4920e-01,  1.3576e-01, -8.0677e-01,  5.4712e-01, -1.1167e-01,\n",
      "          4.0609e-01, -3.5490e-01,  9.4881e-01,  1.9453e-01,  1.0367e+00,\n",
      "          1.0630e-01,  6.8374e-01, -8.0640e-01, -8.8956e-01, -4.5260e-01,\n",
      "          7.3015e-01,  2.3137e-02, -4.1834e-01,  6.1626e-02, -3.7173e-01,\n",
      "         -3.0728e-01, -2.2293e-01,  3.4170e-01, -8.1231e-03,  9.5871e-01,\n",
      "          8.4877e-01, -2.1404e-01, -1.0495e-02,  4.6395e-01, -4.7383e-01,\n",
      "         -5.1991e-01, -1.5740e-01,  7.7627e-02,  9.0971e-02, -3.2028e-01,\n",
      "          6.8933e-01, -1.9736e-01, -3.7317e-01, -6.9210e-01, -1.3841e+00,\n",
      "         -7.5335e-01,  4.0102e-01, -4.8846e-01,  9.2792e-01,  3.8118e-01,\n",
      "         -4.8217e-01, -4.8986e-01,  2.8599e-01,  1.9847e-01,  7.2520e-01,\n",
      "         -4.2452e-01, -6.1007e-02,  4.3399e-01, -4.2560e-01, -1.0127e+00,\n",
      "          4.4607e-01,  1.1575e-01,  1.1406e+00, -4.0180e-01,  7.6445e-01,\n",
      "         -7.8485e-02,  5.4494e-01,  1.0779e-01, -1.0641e-01,  7.1434e-02,\n",
      "         -9.5674e-03,  1.7382e-01,  5.5459e-02, -1.1231e-01,  1.9052e-01,\n",
      "          4.8358e-01, -4.8431e-01,  5.6837e-01,  2.0451e-01, -4.0775e-01,\n",
      "         -1.6378e-02,  2.6348e-01,  7.5581e-01,  3.8352e-01, -6.1829e-01,\n",
      "          2.5028e-01, -2.1398e-01,  3.8165e-02, -6.8484e-01,  5.0376e-01,\n",
      "          1.0260e+00, -3.6676e-01, -4.6066e-01, -7.3693e-01,  9.7606e-02,\n",
      "          8.5941e-01,  4.0177e-01,  2.2392e-01, -1.1870e+00, -7.7953e-02,\n",
      "         -6.8192e-02,  8.2692e-01,  1.3197e-01,  1.0464e+00,  6.7507e-01,\n",
      "          4.7268e-01,  4.5881e-01, -1.1954e-01,  6.0268e-01,  1.1154e-01,\n",
      "         -4.4317e-01,  4.9942e-01, -2.7134e-01, -1.0526e+00,  3.1787e-01,\n",
      "         -2.6261e-01,  3.0495e-01, -7.2827e-01, -6.6663e-01,  1.6298e-02,\n",
      "         -6.2447e-01,  1.3370e-01, -7.9396e-01, -5.9655e-01, -7.7136e-01,\n",
      "         -1.7946e-01, -5.2139e-02, -3.9260e-01,  9.2408e-01,  1.7727e-01,\n",
      "          4.6362e-02, -3.4111e-01,  8.5259e-02,  9.4235e-01, -2.4938e-04,\n",
      "          9.2005e-02, -2.3997e-01,  1.4754e-01, -1.1318e+00, -6.4216e-03,\n",
      "         -3.5196e-01, -1.7930e+00,  5.6501e-03,  3.2774e-01, -4.2935e-02,\n",
      "          1.0987e+00, -3.3474e-01,  6.7948e-03, -4.4960e-01,  2.0314e-01,\n",
      "          3.3211e-01,  8.0450e-01,  7.7304e-01,  2.2860e-01,  7.0497e-01,\n",
      "          5.4354e-01,  4.2701e-01, -5.6043e-01,  8.7371e-01, -4.2804e-01,\n",
      "         -4.5927e-02,  3.9492e-01, -8.7598e-01,  1.3763e+00,  4.8380e-02,\n",
      "         -7.8379e-01, -2.6097e-01,  1.2183e-01, -3.3678e-01, -2.9865e-01,\n",
      "         -7.9920e-01,  1.1995e-02,  1.4948e-01,  1.4018e+00,  2.4179e-01,\n",
      "         -7.2850e-01, -3.0470e-01,  6.2438e-01,  8.2036e-01, -1.9586e-01,\n",
      "         -4.3931e-01, -3.2563e-01,  2.8713e-01, -1.1279e-01,  4.8701e-01,\n",
      "          8.7457e-01,  3.3254e-01,  7.9572e-01, -2.7574e-01,  3.3843e-01,\n",
      "         -3.7023e-01,  4.2413e-01,  7.9373e-01, -7.0527e-01, -5.6351e-01,\n",
      "         -7.0946e-01, -3.5626e-01,  6.7206e-03,  3.1803e-01, -5.5306e-01,\n",
      "         -6.6944e-01, -1.1366e-01,  7.8181e-01, -9.6623e-01,  2.5115e-01,\n",
      "          9.0755e-02, -4.8891e-01, -3.0501e-01,  4.3441e-01, -1.2549e-01,\n",
      "         -3.8588e-01,  8.2266e-01, -3.8150e-01,  2.1365e-01,  6.1021e-01,\n",
      "          2.6689e-01, -1.2015e+00,  6.5474e-01,  2.6402e-01,  2.5276e-01,\n",
      "          9.0072e-02, -9.9313e-02, -1.0391e-01,  7.4550e-01,  1.2169e+00,\n",
      "         -3.9028e-02, -4.2370e-02,  6.7441e-02,  7.4047e-02, -3.1478e-01,\n",
      "          1.8473e-01,  5.6984e-02, -4.6117e-02,  4.9041e-01,  5.2034e-02,\n",
      "          6.7412e-01, -4.7878e-01, -1.0013e+00,  5.9731e-01,  1.0911e+00,\n",
      "          4.2825e-01, -6.1408e-01,  4.1811e-01, -4.0881e-01, -2.7495e-01,\n",
      "          7.3143e-01, -2.7322e-01,  9.2892e-02,  2.9261e-01,  1.1456e+00,\n",
      "          6.1066e-01,  8.7377e-01,  8.1947e-01, -2.1019e+00, -1.2951e-01,\n",
      "          3.3022e-01,  2.2071e-01, -1.0789e+00, -1.5294e-01, -8.1223e-01,\n",
      "         -4.9522e-02,  1.0956e-01,  5.2188e-01, -5.8393e-01,  4.7710e-01,\n",
      "          5.8172e-01, -9.0736e-01,  1.2231e-02, -1.6470e-02, -2.4821e-01,\n",
      "         -3.0571e-01, -2.8550e-01, -4.4148e-01,  1.0116e+00, -1.9518e-01,\n",
      "          1.0892e-04,  4.9413e-01,  1.2365e+00, -3.0653e-01,  1.5042e-01,\n",
      "         -3.2511e-01, -3.9503e-01, -1.3217e+00,  2.4967e-01, -3.5227e-01,\n",
      "         -6.0391e-01, -9.8550e-01, -2.0504e-01,  7.3906e-01,  9.5475e-01,\n",
      "         -3.4602e-02, -1.2468e+00, -6.4681e-01, -4.3830e-01, -2.1264e-01,\n",
      "         -1.5816e-01,  1.2371e+00, -1.2191e+00, -6.7146e-01,  2.5108e-01,\n",
      "         -8.5097e-02,  5.7625e-01,  4.3158e-01, -1.8215e-01,  8.5773e-02,\n",
      "         -8.6839e-01,  1.2532e-01, -7.0834e-02,  4.3797e-01, -2.4433e-01,\n",
      "         -6.4906e-01,  3.6459e-01,  9.8878e-01,  1.2471e+00,  2.5428e-01,\n",
      "          6.0909e-01,  8.1321e-01,  2.3426e-01,  4.3050e-01, -1.2461e+00,\n",
      "          3.5698e-01,  2.9817e-01, -1.9016e-02,  4.5027e-01, -2.0396e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T09:09:26.745488Z",
     "start_time": "2025-07-26T09:08:52.076146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PRETRAİNED MODEL \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pretrain_vit_weights=torchvision.models.ViT_B_16_Weights\n",
    "pretrain_vit=torchvision.models.vit_b_16(weights=pretrain_vit_weights).to(device)\n",
    "\n",
    "for param in pretrain_vit.parameters():\n",
    "    param.requires_grad=False\n",
    "    \n",
    "pretrain_vit\n"
   ],
   "id": "d9f491af3b374842",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /Users/tahay/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330M/330M [00:32<00:00, 10.6MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T09:12:15.209340Z",
     "start_time": "2025-07-26T09:12:14.971237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes=1000\n",
    "pretrain_vit.heads=nn.Linear(in_features=768,out_features=num_classes,bias=True).to(device)\n",
    "summary(model=pretrain_vit,\n",
    "        input_size=(1,3,224,224),\n",
    "        col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ],
   "id": "6f13bcb8092cb80b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 1000]            768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "├─Linear (heads)                                             [1, 768]             [1, 1000]            769,000              True\n",
       "============================================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 769,000\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (Units.MEGABYTES): 173.23\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 336.96\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1323aa1ea97058ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
